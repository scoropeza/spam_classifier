{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Email Classifier with AWS SageMaker Pipelines\n",
    "\n",
    "The purpose of this notebook is to create and train a model that can predict, with high accuracy, whether a given email is a spam or not (ham). This is a binary classification problem that involves Natural Language Processing (NLP).\n",
    "\n",
    "I will be using the <a href='http://www2.aueb.gr/users/ion/data/enron-spam/'>enron spam email dataset</a>.\n",
    "\n",
    "Contents:\n",
    "\n",
    "* 1. [Installing and Importing Dependencies](#libraries)\n",
    "* 2. [Setup SageMaker Environment](#libraries)\n",
    "* 3. [SageMaker Data Pipeline](#pipeline)\n",
    "    * 3.1. [Define Parameters](#pipeline_parameters)\n",
    "    * 3.2  [Loading the Dataset](#pipeline_dataset)\n",
    "    * 3.3. [Data Pre-Processing and Train/Test Split](#pipeline_preprocessing)\n",
    "    * 3.4. [Train Model](#pipeline_train)\n",
    "    * 3.5. [Create Model](#pipeline_create)\n",
    "    * 3.6. [Deploy Model](#pipeline_deploy)\n",
    "    * 3.7. [Register Model](#pipeline_register)\n",
    "    * 3.8. [Pipeline Creation and Execution](#pipeline_execution)\n",
    "* 4. [Model Predictions](#pipeline_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"libraries\">Installing and Importing Dependencies</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook makes use of the following dependencies:\n",
    "* <a href='https://www.nltk.org/'>NLTK</a> - Natural Language Toolkit for Python\n",
    "* <a href='https://scikit-learn.org/'>Scikit-learn</a> - Preprocessing and Machine Learning\n",
    "* <a href='https://pandas.pydata.org/'>Pandas</a> - Data analysis and visualization\n",
    "* <a href='https://numpy.org/'>Numpy</a> - Scientific computing\n",
    "* <a href='https://seaborn.pydata.org/'>Seaborn</a> - Statistical data visualization\n",
    "\n",
    "**This notebook is meant to work on an AWS SageMaker environment (JupyterLab or SageMaker Studio).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: numpy==1.19.5 in /opt/conda/lib/python3.7/site-packages (1.19.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.7/site-packages (0.10.0)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.7/site-packages (from seaborn) (1.4.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.7/site-packages (from seaborn) (1.3.5)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.7/site-packages (from seaborn) (3.1.3)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.7/site-packages (from seaborn) (1.19.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (2.4.6)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.23->seaborn) (2019.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.2->seaborn) (59.5.0)\n",
      "Installing collected packages: seaborn\n",
      "  Attempting uninstall: seaborn\n",
      "    Found existing installation: seaborn 0.10.0\n",
      "    Uninstalling seaborn-0.10.0:\n",
      "      Successfully uninstalled seaborn-0.10.0\n",
      "Successfully installed seaborn-0.11.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install and download required modules for NLTK\n",
    "# ONLY RUN THIS ONCE\n",
    "# !conda install nltk #This installs nltk in a conda environment\n",
    "# %pip install nltk #This installs nltk using pip\n",
    "# %pip install pandas --upgrade\n",
    "# %pip install numpy==1.19.5\n",
    "# %pip install seaborn --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker version: 2.70.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "import os\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import tarfile\n",
    "from sklearn.datasets import load_files\n",
    "#Exploratory Data Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "sns.color_palette(\"Spectral\")\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "#Feature Engineering - Text pre-processing\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#Feature Engineering - Vectorization with Bag of words + TF-IDF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# Model Training\n",
    "from sklearn import set_config\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB # Naive Bayes\n",
    "from sklearn.linear_model import SGDClassifier # SVM Linear classifier with SGD\n",
    "from sklearn.neighbors import KNeighborsClassifier # K-nearest neighbors\n",
    "from sklearn.ensemble import RandomForestClassifier # Random Forest\n",
    "from sklearn.pipeline import Pipeline\n",
    "# AWS Libraries\n",
    "import boto3\n",
    "import sagemaker\n",
    "nltk.download('stopwords') #Download the necessary datasets\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "print(f'SageMaker version: {sagemaker.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"libraries\">Setup SageMaker Environment</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution role and default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-1-926549670619\n"
     ]
    }
   ],
   "source": [
    "#import necessary execution role so that you can read from S3 buckets\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "#source default session parameters (region, default S3 bucket etc)\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "sagemaker_client = boto3.client(\"sagemaker-runtime\")\n",
    "default_bucket = 'enron-spam-classifier'\n",
    "print(sagemaker_session.default_bucket())\n",
    "prefix = 'spam-classifier-sagemaker-pipeline'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"pipeline\">SageMaker Data Pipeline</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"pipeline_parameters\">Define Parameters</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.parameters import (ParameterInteger, ParameterString)\n",
    "\n",
    "#specify location of input data\n",
    "input_data_uri = f\"s3://{default_bucket}/{prefix}/data/emails.csv\"\n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=input_data_uri,\n",
    ")\n",
    "\n",
    "#specify default number of instances for processing step\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "\n",
    "#specify default instance type for processing step\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=\"ml.m4.xlarge\"\n",
    ")\n",
    "\n",
    "#specify default instance type for training step\n",
    "train_instance_type = ParameterString(\n",
    "    name=\"TrainingInstanceType\",\n",
    "    default_value=\"ml.m4.xlarge\",\n",
    ")\n",
    "\n",
    "#specify default model approval mode\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\",\n",
    "    default_value=\"Approved\"\n",
    ")\n",
    "print(f\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"pipeline_dataset\">Loading the dataset</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: ../code\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Create folder structure where code will be stored\n",
    "code_folder_path = \"../code\"\n",
    "code_folder = Path(code_folder_path)\n",
    "try:\n",
    "    code_folder.mkdir(parents=True, exist_ok=False)\n",
    "except FileExistsError as e:\n",
    "    print(f\"Directory already exists: {code_folder_path}\")\n",
    "print(f\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../code/dataset_loading.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../code/dataset_loading.py\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import tarfile\n",
    "from sklearn.datasets import load_files\n",
    "from pathlib import Path\n",
    "import boto3\n",
    "\n",
    "print(f\"Importing dependencies from requirements.txt...\")\n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\", \"sagemaker\",\n",
    "])\n",
    "\n",
    "import sagemaker\n",
    "\n",
    "print(f\"Setting AWS_DEFAULT_REGION environment variable...\")\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'\n",
    "\n",
    "# Function to create an S3 Bucket\n",
    "def create_bucket(bucket_name, region=None):\n",
    "    \"\"\"Create an S3 bucket in a specified region\n",
    "\n",
    "    If a region is not specified, the bucket is created in the S3 default\n",
    "    region (us-east-1).\n",
    "\n",
    "    :param bucket_name: Bucket to create\n",
    "    :param region: String region to create bucket in, e.g., 'us-west-2'\n",
    "    :return: True if bucket created, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # Create bucket\n",
    "    try:\n",
    "        if region is None:\n",
    "            s3_client = boto3.client('s3')\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            s3_client = boto3.client('s3', region_name=region)\n",
    "            location = {'LocationConstraint': region}\n",
    "            s3_client.create_bucket(Bucket=bucket_name,\n",
    "                                    CreateBucketConfiguration=location)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "#source default session parameters (region, default S3 bucket etc)\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "sagemaker_client = boto3.client(\"sagemaker-runtime\")\n",
    "default_bucket = 'enron-spam-classifier'\n",
    "prefix = 'spam-classifier-sagemaker-pipeline'\n",
    "\n",
    "# Create folder structure where the dataset will be stored\n",
    "dataset_folder_path = '/opt/ml/processing/input/spam_dataset'\n",
    "dataset_folder = Path(dataset_folder_path)\n",
    "upload_files = False\n",
    "try:\n",
    "    dataset_folder.mkdir(parents=True, exist_ok=False)\n",
    "    upload_files = True\n",
    "except FileExistsError as e:\n",
    "    print(f\"Directory already exists: {dataset_folder_path}\")\n",
    "    \n",
    "# Download the dataset from the source location\n",
    "print(f\"Downloading the dataset from the source location...\")\n",
    "if upload_files:\n",
    "    for i in range(1, 7):\n",
    "        urllib.request.urlretrieve(\"http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron\"+str(i)+\".tar.gz\", dataset_folder_path+\"/enron\"+str(i)+\".tar.gz\")\n",
    "        print(f\"Extracting enron\"+str(i)+\".tar.gz...\")\n",
    "        file = tarfile.open(str(Path(dataset_folder_path+'/enron'+str(i)+'.tar.gz').absolute()))\n",
    "        file.extractall(dataset_folder_path)\n",
    "        # Remove Tar file\n",
    "        file_to_rem = Path(dataset_folder_path+'/enron'+str(i)+'.tar.gz')\n",
    "        file_to_rem.unlink()\n",
    "\n",
    "print(f\"Loading dataset...\")\n",
    "X, y = [], []\n",
    "for subdir in os.listdir(dataset_folder_path):\n",
    "    if os.path.isdir and subdir != '.DS_Store' and subdir != '.ipynb_checkpoints':\n",
    "        #and subdir  == 'enron1'\n",
    "        print(f\"Loading files from path {os.path.join(dataset_folder_path, subdir)}\")\n",
    "        email_dataset = load_files(os.path.join(dataset_folder_path, subdir))\n",
    "        X = np.append(X, email_dataset.data)\n",
    "        y = np.append(y, email_dataset.target)\n",
    "\n",
    "# Create pandas dataframe for further analysis and manipulation\n",
    "print(\"Creating pandas dataframe for further analysis and manipulation...\")\n",
    "emails = pd.DataFrame(columns=['label', 'label_name', 'text'])\n",
    "emails['label'] = [y for y in y]\n",
    "emails['label_name'] = emails.apply (lambda row: 'ham' if row.label == 0 else 'spam', axis=1)\n",
    "emails['text'] = [x for x in X]\n",
    "\n",
    "#Save dataframe as CSV\n",
    "print(f\"Saving pandas dataframe to local csv file...\")\n",
    "dataset_folder_path = '/opt/ml/processing/input/spam_dataset/pandas'\n",
    "dataset_folder = Path(dataset_folder_path)\n",
    "try:\n",
    "    dataset_folder.mkdir(parents=True, exist_ok=False)\n",
    "    upload_files = True\n",
    "except FileExistsError as e:\n",
    "    print(f\"Directory already exists: {dataset_folder_path}\")\n",
    "emails.to_csv(str(Path(dataset_folder_path+'/emails.csv').absolute()),index=False)\n",
    "\n",
    "#create bucket\n",
    "print(f\"Creating bucket {default_bucket}...\")\n",
    "create_bucket(default_bucket)\n",
    "\n",
    "#upload the data to your default S3 bucket or another S3 bucket of your choosing\n",
    "print(f\"Uploading csv to S3...\")\n",
    "base_uri = f\"s3://{default_bucket}/{prefix}/data\"\n",
    "input_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=dataset_folder_path,\n",
    "    desired_s3_uri=base_uri,\n",
    ")\n",
    "print(input_data_uri)\n",
    "\n",
    "print(f\"done!\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "framework_version = \"0.23-1\"\n",
    "\n",
    "sklearn_processor_dataset_loading = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"sklearn-text-datasetloading\",\n",
    "    role=role,\n",
    ")\n",
    "print(f\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"pipeline_preprocessing\">Data Pre-Processing and Train/Test Split</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../code/requirements.txt\n",
    "nltk\n",
    "pandas == 1.3.5\n",
    "numpy == 1.19.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../code/text_preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../code/text_preprocessing.py\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Importing dependencies from requirements.txt...\")\n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\", \"nltk\",\n",
    "])\n",
    "# subprocess.check_call([\n",
    "#     sys.executable, \"-m\", \"pip\", \"install\", \"-r\",\n",
    "#     \"/opt/ml/processing/input/code/requirements.txt\",\n",
    "# ])\n",
    "\n",
    "#Feature Engineering - Text pre-processing\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#Feature Engineering - Vectorization with Bag of words + TF-IDF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "print(f\"Downloading nltk corpora...\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "#Text Pre-Processing Function\n",
    "def email_text_preprocessing(document_text,text_normalization='none'):\n",
    "    '''\n",
    "    :param str document: document to be pre-processed in text format.\n",
    "    :param str text_normalization: It defines the type of normalization to be applied, possible values are: 'none', 'stemming', 'lemmatization'\n",
    "    1. Remove b' characters that appear at the beggining of each email\n",
    "    2. Remove special characters\n",
    "    3. Remove punctuation marks\n",
    "    4. Remove stopwords\n",
    "    5. Stem, Lemmatize text, or do nothing else.\n",
    "    '''\n",
    "    alt_text = re.sub(r'^b\\'', '', document_text)\n",
    "    alt_text = re.sub(r'\\\\r\\\\n', ' ', alt_text)\n",
    "    alt_text = re.sub(r'\\s+',' ',alt_text)\n",
    "    no_punct = [char for char in alt_text if char not in string.punctuation]\n",
    "    no_punct = ''.join(no_punct)\n",
    "    no_stopwords = [word for word in no_punct.split() if word.lower() not in stopwords.words('english')]\n",
    "    res_text = None\n",
    "    if text_normalization == 'stemming':\n",
    "        stemmer = PorterStemmer()\n",
    "        stemmed = [stemmer.stem(word) for word in no_stopwords]\n",
    "        stemmed = ' '.join(stemmed)\n",
    "        res_text = stemmed\n",
    "    elif text_normalization == 'lemmatization':\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized = [lemmatizer.lemmatize(word) for word in no_stopwords]\n",
    "        lemmatized = ' '.join(lemmatized)\n",
    "        res_text = lemmatized\n",
    "    else:\n",
    "        res_text = ' '.join(no_stopwords)\n",
    "\n",
    "    return res_text\n",
    "\n",
    "#Load dataframe from csv\n",
    "print(f\"Creating pandas dataframe from local csv file...\")\n",
    "dataset_folder_path = '/opt/ml/processing/input'\n",
    "dataset_folder = Path(dataset_folder_path)\n",
    "emails = pd.read_csv(str(Path(dataset_folder_path+'/emails.csv').absolute()))\n",
    "\n",
    "#Create new column in data frame with pre-proccesed text (lemmatized)\n",
    "print(f\"Creating new column in data frame with pre-proccesed text (lemmatized)...\")\n",
    "emails['preprocessed_text_lemmatized'] = ''\n",
    "emails.astype({'preprocessed_text_lemmatized': 'string'}).dtypes\n",
    "emails['preprocessed_text_lemmatized'] = emails['text']\n",
    "emails['preprocessed_text_lemmatized'] = emails['preprocessed_text_lemmatized'].apply(lambda x: email_text_preprocessing(str(x), text_normalization='lemmatization'))\n",
    "\n",
    "#Create column in format expected by the BlazingText Algorithm\n",
    "print(f\"Creating column in format expected by the BlazingText Algorithm...\")\n",
    "emails['blazingtext_lemmatized'] = ''\n",
    "emails.astype({'blazingtext_lemmatized': 'string'}).dtypes\n",
    "emails['blazingtext_lemmatized'] = '__label__' + emails['label_name'].astype(str) + ' ' + emails['preprocessed_text_lemmatized']\n",
    "\n",
    "#Create train:test split (train: 70%, validation: 25%, test: 5%)\n",
    "print(f\"Creating train:test split (train: 70%, validation: 25%, test: 5%)...\")\n",
    "train, validation, test = np.split(emails, [int(0.7 * len(emails)), int(0.85 * len(emails))])\n",
    "                     \n",
    "#Create Series datasets for BlazingText format\n",
    "train = train['blazingtext_lemmatized']\n",
    "validation = validation['blazingtext_lemmatized']\n",
    "test = test['blazingtext_lemmatized']\n",
    "                     \n",
    "#Save datasets\n",
    "print(f\"Saving datasets...\")\n",
    "base_dir = \"/opt/ml/processing\"\n",
    "pd.DataFrame(train).to_csv(str(Path(base_dir+'/train/train.csv').absolute()), header=False, index=False)\n",
    "pd.DataFrame(validation).to_csv(str(Path(base_dir+'/validation/validation.csv').absolute()), header=False, index=False)\n",
    "pd.DataFrame(test).to_csv(str(Path(base_dir+'/test/test.csv').absolute()), header=False, index=False)\n",
    "\n",
    "print(f\"Number of emails in the training dataset: {train.shape[0]}\")\n",
    "print(f\"Number of emails in the validation set: {validation.shape[0]}\")\n",
    "print(f\"Number of emails in the test set: {test.shape[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "framework_version = \"0.23-1\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"sklearn-text-preprocessing\",\n",
    "    role=role,\n",
    ")\n",
    "print(f\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup data loading and text pre-processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "default_bucket = 'enron-spam-classifier'\n",
    "prefix = 'spam-classifier-sagemaker-pipeline'\n",
    "base_job_name=\"sklearn-text-preprocessing\"\n",
    "output_destination_base = \"s3://\"+default_bucket+\"/\"+prefix+\"/\"+base_job_name+\"/output/\"\n",
    "\n",
    "s3_client.upload_file(Filename='../code/dataset_loading.py', Bucket=default_bucket, Key=f'{prefix}/code/dataset_loading.py')\n",
    "dataset_loading_script_uri = f's3://{default_bucket}/{prefix}/code/dataset_loading.py'\n",
    "\n",
    "s3_client.upload_file(Filename='../code/requirements.txt', Bucket=default_bucket, Key=f'{prefix}/code/requirements.txt')\n",
    "requirements_script_uri = f's3://{default_bucket}/{prefix}/code/requirements.txt'\n",
    "\n",
    "s3_client.upload_file(Filename='../code/text_preprocessing.py', Bucket=default_bucket, Key=f'{prefix}/code/text_preprocessing.py')\n",
    "preprocess_script_uri = f's3://{default_bucket}/{prefix}/code/text_preprocessing.py'\n",
    "\n",
    "dataload_step = ProcessingStep(\n",
    "    name=\"BlazingTextDatasetLoadingStep\",\n",
    "    processor=sklearn_processor_dataset_loading,\n",
    "    inputs=[],\n",
    "    outputs=[\n",
    "        #ProcessingOutput(output_name=\"dataset\", source=\"/opt/ml/processing/input/spam_dataset/pandas\", destination=output_destination_base+\"pandas\")\n",
    "    ],\n",
    "    code=dataset_loading_script_uri,\n",
    ")\n",
    "\n",
    "process_step = ProcessingStep(\n",
    "    name=\"BlazingTextPreProcessingStep\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\")\n",
    "        # Send requirements.txt\n",
    "#         ProcessingInput(source=requirements_script_uri, destination=\"/opt/ml/processing/input/code\", input_name=\"requirements.txt\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\", destination=output_destination_base+\"train\"),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\", destination=output_destination_base+\"validation\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\", destination=output_destination_base+\"test\"),\n",
    "    ],\n",
    "    code=preprocess_script_uri,\n",
    ")\n",
    "print(f\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"pipeline_train\">Train Model</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "# set up estimator:\n",
    "\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "default_bucket = 'enron-spam-classifier'\n",
    "prefix = 'spam-classifier-sagemaker-pipeline'\n",
    "base_job_name=\"sklearn-text-preprocessing\"\n",
    "\n",
    "bt_estimator = Estimator(\n",
    "    role=role,\n",
    "    instance_type=train_instance_type,\n",
    "    instance_count=1,\n",
    "    image_uri=sagemaker.image_uris.retrieve(\"blazingtext\", region),\n",
    "    output_path=f's3://{default_bucket}/{prefix}/training_jobs',\n",
    "    base_job_name='bt-model-estimator',\n",
    "    input_mode = 'File'\n",
    ")\n",
    "\n",
    "#for more info on hyperparameters, see: https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html\n",
    "bt_estimator.set_hyperparameters(mode=\"supervised\",\n",
    "                                 epochs=50,\n",
    "                                 learning_rate=0.02,\n",
    "                                 min_count=2,\n",
    "                                 early_stopping=True,\n",
    "                                 patience=5,\n",
    "                                 min_epochs=20,\n",
    "                                 word_ngrams=4\n",
    "                                )\n",
    "print(f\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup model training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "# set up model training step\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "s3_data_base_uri=f\"s3://{default_bucket}/{prefix}/{base_job_name}/output/\"\n",
    "\n",
    "train_step = TrainingStep(\n",
    "    name='BlazingTextTrainingStep',\n",
    "    estimator=bt_estimator,\n",
    "    inputs={\n",
    "        'train': sagemaker.inputs.TrainingInput(\n",
    "            #s3_data=process_step.properties.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri,\n",
    "            s3_data=str(s3_data_base_uri)+\"train/train.csv\",\n",
    "            content_type=\"text/csv\"\n",
    "        ),\n",
    "        'validation': sagemaker.inputs.TrainingInput(\n",
    "            #s3_data=process_step.properties.ProcessingOutputConfig.Outputs['validation'].S3Output.S3Uri,\n",
    "            s3_data=str(s3_data_base_uri)+\"validation/validation.csv\",\n",
    "            content_type=\"text/csv\"\n",
    "        )\n",
    "    }\n",
    ")\n",
    "print(f\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"pipeline_create\">Create Model</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Create Model Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "\n",
    "model = sagemaker.model.Model(\n",
    "    name='spam-classifier-blazingtext-model',\n",
    "    image_uri=train_step.properties.AlgorithmSpecification.TrainingImage,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role\n",
    ")\n",
    "\n",
    "inputs = sagemaker.inputs.CreateModelInput(\n",
    "    instance_type=\"ml.m4.xlarge\"\n",
    ")\n",
    "\n",
    "create_model_step = CreateModelStep(\n",
    "    name=\"BlazingTextModelCreationStep\",\n",
    "    model=model,\n",
    "    inputs=inputs\n",
    ")\n",
    "print(f\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"pipeline_deploy\">Deploy Model</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../code/deploy_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../code/deploy_model.py\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "import argparse\n",
    "\n",
    "\n",
    "# Parse argument variables passed via the DeployModel processing step\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model-name', type=str)\n",
    "parser.add_argument('--region', type=str)\n",
    "parser.add_argument('--endpoint-instance-type', type=str)\n",
    "parser.add_argument('--endpoint-name', type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "region = args.region\n",
    "boto3.setup_default_session(region_name=region)\n",
    "sagemaker_boto_client = boto3.client('sagemaker')\n",
    "\n",
    "# truncate name per sagameker length requirememnts (63 char max) if necessary\n",
    "endpoint_config_name = f'{args.endpoint_name}-config-{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "# create new endpoint config file\n",
    "create_ep_config_response = sagemaker_boto_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': args.endpoint_instance_type,\n",
    "        'InitialVariantWeight': 1,\n",
    "        'InitialInstanceCount': 1,\n",
    "        'ModelName': args.model_name,\n",
    "        'VariantName': 'AllTraffic'\n",
    "        }])\n",
    "\n",
    "print(\"ModelName: {}\".format(args.model_name))\n",
    "\n",
    "# create endpoint if model endpoint does not already exist, otherwise update the endpoint\n",
    "try:\n",
    "    create_endpoint_response = sagemaker_boto_client.create_endpoint(\n",
    "        EndpointName=args.endpoint_name,\n",
    "        EndpointConfigName=endpoint_config_name\n",
    "    )\n",
    "except:\n",
    "    create_endpoint_response = sagemaker_boto_client.update_endpoint(\n",
    "        EndpointName=args.endpoint_name,\n",
    "        EndpointConfigName=endpoint_config_name\n",
    "    )\n",
    "\n",
    "endpoint_info = sagemaker_boto_client.describe_endpoint(EndpointName=args.endpoint_name)\n",
    "endpoint_status = endpoint_info['EndpointStatus']\n",
    "\n",
    "while endpoint_status != 'InService':\n",
    "    endpoint_info = sagemaker_boto_client.describe_endpoint(EndpointName=args.endpoint_name)\n",
    "    endpoint_status = endpoint_info['EndpointStatus']\n",
    "    print('Endpoint status:', endpoint_status)\n",
    "    if endpoint_status != 'InService':\n",
    "        time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup deploy model step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "s3_client.upload_file(Filename='../code/deploy_model.py', Bucket=default_bucket, Key=f'{prefix}/code/deploy_model.py')\n",
    "deploy_model_script_uri = f's3://{default_bucket}/{prefix}/code/deploy_model.py'\n",
    "pipeline_endpoint_name = 'spam-classifier-btext'\n",
    "\n",
    "deployment_instance_type = \"ml.m4.xlarge\"\n",
    "\n",
    "deploy_model_processor = SKLearnProcessor(\n",
    "    framework_version='0.23-1',\n",
    "    role=role,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    instance_count=1,\n",
    "    volume_size_in_gb=60,\n",
    "    base_job_name='sklearn-deploy-model',\n",
    "    sagemaker_session=sagemaker_session)\n",
    "\n",
    "deploy_step = ProcessingStep(\n",
    "    name='BlazingTextModelDeploymentStep',\n",
    "    processor=deploy_model_processor,\n",
    "    job_arguments=[\n",
    "        \"--model-name\", create_model_step.properties.ModelName,\n",
    "        \"--region\", region,\n",
    "        \"--endpoint-instance-type\", deployment_instance_type,\n",
    "        \"--endpoint-name\", pipeline_endpoint_name\n",
    "    ],\n",
    "    code=deploy_model_script_uri)\n",
    "print(f\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"pipeline_register\">Register Model</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Register Model Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "register_step = RegisterModel(\n",
    "    name=\"BlazingTextModelRegistrationStep\",\n",
    "    estimator=bt_estimator,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=prefix,\n",
    "    approval_status=model_approval_status,\n",
    ")\n",
    "print(f\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"pipeline_execution\">Pipeline Creation and Execution</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "#run full pipeline\n",
    "steps_full = [\n",
    "                dataload_step,\n",
    "                process_step,\n",
    "                train_step,\n",
    "                create_model_step,\n",
    "                deploy_step,\n",
    "                register_step]\n",
    "\n",
    "#run data loading step\n",
    "steps_dataset_loading = [dataload_step]\n",
    "\n",
    "#run data processing step\n",
    "steps_preprocessing = [process_step]\n",
    "\n",
    "#run full pipeline\n",
    "steps_train_deployment = [\n",
    "                train_step,\n",
    "                create_model_step,\n",
    "                deploy_step,\n",
    "                register_step]\n",
    "\n",
    "pipeline_name = 'BlazingTextPipeline'\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        processing_instance_count,\n",
    "        train_instance_type,\n",
    "        model_approval_status,\n",
    "        input_data\n",
    "    ],\n",
    "    steps=steps_train_deployment, #switch to steps_preprocessing if you would like to run only the data processing step\n",
    ")\n",
    "print(f\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Version': '2020-12-01',\n",
       " 'Metadata': {},\n",
       " 'Parameters': [{'Name': 'ProcessingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m4.xlarge'},\n",
       "  {'Name': 'ProcessingInstanceCount', 'Type': 'Integer', 'DefaultValue': 1},\n",
       "  {'Name': 'TrainingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m4.xlarge'},\n",
       "  {'Name': 'ModelApprovalStatus',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'Approved'},\n",
       "  {'Name': 'InputData',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://enron-spam-classifier/spam-classifier-sagemaker-pipeline/data/emails.csv'}],\n",
       " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
       "  'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
       " 'Steps': [{'Name': 'BlazingTextTrainingStep',\n",
       "   'Type': 'Training',\n",
       "   'Arguments': {'AlgorithmSpecification': {'TrainingInputMode': 'File',\n",
       "     'TrainingImage': '811284229777.dkr.ecr.us-east-1.amazonaws.com/blazingtext:1'},\n",
       "    'OutputDataConfig': {'S3OutputPath': 's3://enron-spam-classifier/spam-classifier-sagemaker-pipeline/training_jobs'},\n",
       "    'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "    'ResourceConfig': {'InstanceCount': 1,\n",
       "     'InstanceType': {'Get': 'Parameters.TrainingInstanceType'},\n",
       "     'VolumeSizeInGB': 30},\n",
       "    'RoleArn': 'arn:aws:iam::926549670619:role/service-role/AmazonSageMaker-ExecutionRole-20211029T104971',\n",
       "    'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': 's3://enron-spam-classifier/spam-classifier-sagemaker-pipeline/sklearn-text-preprocessing/output/train/train.csv',\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ContentType': 'text/csv',\n",
       "      'ChannelName': 'train'},\n",
       "     {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': 's3://enron-spam-classifier/spam-classifier-sagemaker-pipeline/sklearn-text-preprocessing/output/validation/validation.csv',\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ContentType': 'text/csv',\n",
       "      'ChannelName': 'validation'}],\n",
       "    'HyperParameters': {'mode': 'supervised',\n",
       "     'epochs': '50',\n",
       "     'learning_rate': '0.02',\n",
       "     'min_count': '2',\n",
       "     'early_stopping': 'True',\n",
       "     'patience': '5',\n",
       "     'min_epochs': '20',\n",
       "     'word_ngrams': '4'},\n",
       "    'ProfilerRuleConfigurations': [{'RuleConfigurationName': 'ProfilerReport-1641092437',\n",
       "      'RuleEvaluatorImage': '503895931360.dkr.ecr.us-east-1.amazonaws.com/sagemaker-debugger-rules:latest',\n",
       "      'RuleParameters': {'rule_to_invoke': 'ProfilerReport'}}],\n",
       "    'ProfilerConfig': {'S3OutputPath': 's3://enron-spam-classifier/spam-classifier-sagemaker-pipeline/training_jobs'}}},\n",
       "  {'Name': 'BlazingTextModelCreationStep',\n",
       "   'Type': 'Model',\n",
       "   'Arguments': {'ExecutionRoleArn': 'arn:aws:iam::926549670619:role/service-role/AmazonSageMaker-ExecutionRole-20211029T104971',\n",
       "    'PrimaryContainer': {'Image': {'Get': 'Steps.BlazingTextTrainingStep.AlgorithmSpecification.TrainingImage'},\n",
       "     'Environment': {},\n",
       "     'ModelDataUrl': {'Get': 'Steps.BlazingTextTrainingStep.ModelArtifacts.S3ModelArtifacts'}}}},\n",
       "  {'Name': 'BlazingTextModelDeploymentStep',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.m5.xlarge',\n",
       "      'InstanceCount': 1,\n",
       "      'VolumeSizeInGB': 60}},\n",
       "    'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3',\n",
       "     'ContainerArguments': ['--model-name',\n",
       "      {'Get': 'Steps.BlazingTextModelCreationStep.ModelName'},\n",
       "      '--region',\n",
       "      'us-east-1',\n",
       "      '--endpoint-instance-type',\n",
       "      'ml.m4.xlarge',\n",
       "      '--endpoint-name',\n",
       "      'spam-classifier-btext'],\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/deploy_model.py']},\n",
       "    'RoleArn': 'arn:aws:iam::926549670619:role/service-role/AmazonSageMaker-ExecutionRole-20211029T104971',\n",
       "    'ProcessingInputs': [{'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://enron-spam-classifier/spam-classifier-sagemaker-pipeline/code/deploy_model.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}]}},\n",
       "  {'Name': 'BlazingTextModelRegistrationStep',\n",
       "   'Type': 'RegisterModel',\n",
       "   'Arguments': {'ModelPackageGroupName': 'spam-classifier-sagemaker-pipeline',\n",
       "    'InferenceSpecification': {'Containers': [{'Image': '811284229777.dkr.ecr.us-east-1.amazonaws.com/blazingtext:1',\n",
       "       'ModelDataUrl': {'Get': 'Steps.BlazingTextTrainingStep.ModelArtifacts.S3ModelArtifacts'}}],\n",
       "     'SupportedContentTypes': ['text/csv'],\n",
       "     'SupportedResponseMIMETypes': ['text/csv'],\n",
       "     'SupportedRealtimeInferenceInstanceTypes': ['ml.t2.medium',\n",
       "      'ml.m5.xlarge'],\n",
       "     'SupportedTransformInstanceTypes': ['ml.m5.xlarge']},\n",
       "    'ModelApprovalStatus': {'Get': 'Parameters.ModelApprovalStatus'}}}]}"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:926549670619:pipeline/blazingtextpipeline',\n",
       " 'ResponseMetadata': {'RequestId': '13b55bad-fcbd-4c1f-914a-b4b349396bf9',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '13b55bad-fcbd-4c1f-914a-b4b349396bf9',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '87',\n",
       "   'date': 'Sun, 02 Jan 2022 03:00:51 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:926549670619:pipeline/blazingtextpipeline',\n",
       " 'PipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:926549670619:pipeline/blazingtextpipeline/execution/cuzv6t71qsvj',\n",
       " 'PipelineExecutionDisplayName': 'execution-1641092456228',\n",
       " 'PipelineExecutionStatus': 'Executing',\n",
       " 'PipelineExperimentConfig': {'ExperimentName': 'blazingtextpipeline',\n",
       "  'TrialName': 'cuzv6t71qsvj'},\n",
       " 'CreationTime': datetime.datetime(2022, 1, 2, 3, 0, 56, 76000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2022, 1, 2, 3, 0, 56, 76000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:926549670619:user-profile/d-aim3ywjn7ssh/samcaso',\n",
       "  'UserProfileName': 'samcaso',\n",
       "  'DomainId': 'd-aim3ywjn7ssh'},\n",
       " 'LastModifiedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:926549670619:user-profile/d-aim3ywjn7ssh/samcaso',\n",
       "  'UserProfileName': 'samcaso',\n",
       "  'DomainId': 'd-aim3ywjn7ssh'},\n",
       " 'ResponseMetadata': {'RequestId': '9e83e3f2-704f-44d5-aa6a-3220ab5ce49c',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '9e83e3f2-704f-44d5-aa6a-3220ab5ce49c',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '800',\n",
       "   'date': 'Sun, 02 Jan 2022 03:00:58 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'StepName': 'BlazingTextModelDeploymentStep',\n",
       "  'StartTime': datetime.datetime(2022, 1, 2, 3, 8, 37, 220000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2022, 1, 2, 3, 16, 15, 936000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'AttemptCount': 0,\n",
       "  'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:926549670619:processing-job/pipelines-cuzv6t71qsvj-blazingtextmodeldepl-yguizugwm9'}}},\n",
       " {'StepName': 'BlazingTextModelRegistrationStep',\n",
       "  'StartTime': datetime.datetime(2022, 1, 2, 3, 8, 35, 964000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2022, 1, 2, 3, 8, 36, 777000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'AttemptCount': 0,\n",
       "  'Metadata': {'RegisterModel': {'Arn': 'arn:aws:sagemaker:us-east-1:926549670619:model-package/spam-classifier-sagemaker-pipeline/3'}}},\n",
       " {'StepName': 'BlazingTextModelCreationStep',\n",
       "  'StartTime': datetime.datetime(2022, 1, 2, 3, 8, 35, 964000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2022, 1, 2, 3, 8, 36, 940000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'AttemptCount': 0,\n",
       "  'Metadata': {'Model': {'Arn': 'arn:aws:sagemaker:us-east-1:926549670619:model/pipelines-cuzv6t71qsvj-blazingtextmodelcrea-joq5srted7'}}},\n",
       " {'StepName': 'BlazingTextTrainingStep',\n",
       "  'StartTime': datetime.datetime(2022, 1, 2, 3, 0, 57, 649000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2022, 1, 2, 3, 8, 35, 413000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'AttemptCount': 0,\n",
       "  'Metadata': {'TrainingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:926549670619:training-job/pipelines-cuzv6t71qsvj-blazingtexttrainings-f1b6iqbzes'}}}]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"pipeline_predictions\">Model Predictions</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function that removes the '__ label __\\<label name\\>' string required by the BlazingText algorithm to use the test dataset emails for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "def remove_label(document_text):\n",
    "    alt_text = re.sub(r'^__label__[a-z]+', '', document_text)\n",
    "    return alt_text\n",
    "\n",
    "print(\"done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grab sample emails from the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful S3 get_object response. Status - 200\n",
      "-----\n",
      "__label__ham bSubject total transfer capability attached total transfer capability ttc july 4 2001 attached outage information reliable time posting attached outage information subject change without notice myrna neeley administrative assistant outage coordination transmission maintenance california independent system operator voice 916 351 2171 fax 916 351 2367 e mail mneeley caiso com ttc 07 04 01 pdf\n",
      "-----\n",
      "__label__spam Subject creative dc cam 3200 z 154 00 creative dc cam 3200 z 154 00 stylishly designed cool chrome silver color ultra compact body creative dc cam 3200 z power packed great feature small scale true effective 3 2 megapixels advanced ccd imaging technology 12 time maximum precision zoom dc cam 3200 z delivers crisp clear image rich vibrant color best digital photo result feature 3 2 megapixels ccd sensor 3 x optical plus 4 x digital zoom mpeg 4 video capture playback audio built speakerintelligent powerful strobe flash 16 mb integrated memory sd expansion slot multi language lcd panel menu date stamping megapixel lcd display memory optical zoom digital zoom 3 2 1 5 16 mb 3 x 4 x visit http www computron com deal one stop distributorjebel ali duty free zonedubai uae www computron com latest clearance sale listing contact sale department limited quantity available selected special detail please send enquiry dealer emirate net aeor contact via www computron com compaq hewlett packard 3 com dell intel iomega epson aopen creative toshiba apc cisco u robotics microsoft canon intellinet targus viewsonic ibm sony lot complaint suggestion contact customerservice computron com tel 971 4 8834464 price u dollar ex work fax 971 4 8834454 jebel ali duty free zone www computron com price availability subject change usa canada u e without notice receive special offer plain text format reply mail request export email considered spam long include contact information remove instruction message intended dealer resellers somehow gotten list error reason would like removed please reply remove subject line message message sent compliance federal legislation commercial e mail h r 4176 section 101 paragraph e 1 bill 1618 title iii passed 105 th u congress logo trademark property respective owner product may exactly shown unsubscribe computron 6 follow link click link copy paste address browser\n",
      "-----\n",
      "__label__spam Subject sale u e usb voip handset w built audio adapter sale u e usb voip handset w built audio adapter 25 00 enjoy stereo music make internet phone call multi audio adapter work sound card well voip device simply plug available usb port included handset make simple make internet phone call slim lightweight design make easy carry anywhere order today computron visit http www computron com deal one stop office td 01 jebel ali duty free zonedubai uae www computron com latest clearance sale listing contact sale department detail please send enquiry dealer emirate net aeor contact via www computron com compaq hewlett packard 3 com dell intel iomega epson aopen creative toshiba apc cisco u robotics microsoft canon intellinet targus viewsonic ibm sony lot complaint suggestion contact customerservice computron com tel 971 4 8834464 price u dollar ex work fax 971 4 8834454 jebel ali duty free zone www computron com price availability subject change usa canada u e without notice receive special offer plain text format reply mail request export email considered spam long include contact information remove instruction message intended dealer resellers somehow gotten list error reason would like removed please reply remove subject line message message sent compliance federal legislation commercial e mail h r 4176 section 101 paragraph e 1 bill 1618 title iii passed 105 th u congress logo trademark property respective owner product may exactly shown unsubscribe computron 8 follow link click link copy paste address browser\n"
     ]
    }
   ],
   "source": [
    "#Read test dataset from S3\n",
    "response = s3_client.get_object(Bucket=default_bucket, Key=\"spam-classifier-sagemaker-pipeline/sklearn-text-preprocessing/output/test/test.csv\")\n",
    "status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "\n",
    "if status == 200:\n",
    "    print(f\"Successful S3 get_object response. Status - {status}\")\n",
    "    test_emails_df = pd.read_csv(response.get(\"Body\"))\n",
    "    #print(test_emails_df)\n",
    "else:\n",
    "    print(f\"Unsuccessful S3 get_object response. Status - {status}\")\n",
    "\n",
    "print('-----')\n",
    "print(test_emails_df.iloc[10][0])\n",
    "print('-----')\n",
    "print(test_emails_df.iloc[20][0])\n",
    "print('-----')\n",
    "print(test_emails_df.iloc[30][0])\n",
    "#emails['blazingtext_lemmatized'] = '__label__'\n",
    "test_emails = [remove_label(str(test_emails_df.iloc[10][0])),\n",
    "              remove_label(str(test_emails_df.iloc[20][0])),\n",
    "              remove_label(str(test_emails_df.iloc[30][0]))]\n",
    "\n",
    "payload = {\"instances\" : test_emails}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function to be used to return model predicitons using the previosly generated SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(payload, endpoint_name, client):\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload),\n",
    "        ContentType='application/json')\n",
    "    predictions = json.loads(response['Body'].read().decode('utf-8'))\n",
    "    return list(zip(payload['instances'], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return Predictions from the SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' bSubject total transfer capability attached total transfer capability ttc july 4 2001 attached outage information reliable time posting attached outage information subject change without notice myrna neeley administrative assistant outage coordination transmission maintenance california independent system operator voice 916 351 2171 fax 916 351 2367 e mail mneeley caiso com ttc 07 04 01 pdf',\n",
       "  {'label': ['__label__ham'], 'prob': [0.993827760219574]}),\n",
       " (' Subject creative dc cam 3200 z 154 00 creative dc cam 3200 z 154 00 stylishly designed cool chrome silver color ultra compact body creative dc cam 3200 z power packed great feature small scale true effective 3 2 megapixels advanced ccd imaging technology 12 time maximum precision zoom dc cam 3200 z delivers crisp clear image rich vibrant color best digital photo result feature 3 2 megapixels ccd sensor 3 x optical plus 4 x digital zoom mpeg 4 video capture playback audio built speakerintelligent powerful strobe flash 16 mb integrated memory sd expansion slot multi language lcd panel menu date stamping megapixel lcd display memory optical zoom digital zoom 3 2 1 5 16 mb 3 x 4 x visit http www computron com deal one stop distributorjebel ali duty free zonedubai uae www computron com latest clearance sale listing contact sale department limited quantity available selected special detail please send enquiry dealer emirate net aeor contact via www computron com compaq hewlett packard 3 com dell intel iomega epson aopen creative toshiba apc cisco u robotics microsoft canon intellinet targus viewsonic ibm sony lot complaint suggestion contact customerservice computron com tel 971 4 8834464 price u dollar ex work fax 971 4 8834454 jebel ali duty free zone www computron com price availability subject change usa canada u e without notice receive special offer plain text format reply mail request export email considered spam long include contact information remove instruction message intended dealer resellers somehow gotten list error reason would like removed please reply remove subject line message message sent compliance federal legislation commercial e mail h r 4176 section 101 paragraph e 1 bill 1618 title iii passed 105 th u congress logo trademark property respective owner product may exactly shown unsubscribe computron 6 follow link click link copy paste address browser',\n",
       "  {'label': ['__label__spam'], 'prob': [0.9736527800559998]}),\n",
       " (' Subject sale u e usb voip handset w built audio adapter sale u e usb voip handset w built audio adapter 25 00 enjoy stereo music make internet phone call multi audio adapter work sound card well voip device simply plug available usb port included handset make simple make internet phone call slim lightweight design make easy carry anywhere order today computron visit http www computron com deal one stop office td 01 jebel ali duty free zonedubai uae www computron com latest clearance sale listing contact sale department detail please send enquiry dealer emirate net aeor contact via www computron com compaq hewlett packard 3 com dell intel iomega epson aopen creative toshiba apc cisco u robotics microsoft canon intellinet targus viewsonic ibm sony lot complaint suggestion contact customerservice computron com tel 971 4 8834464 price u dollar ex work fax 971 4 8834454 jebel ali duty free zone www computron com price availability subject change usa canada u e without notice receive special offer plain text format reply mail request export email considered spam long include contact information remove instruction message intended dealer resellers somehow gotten list error reason would like removed please reply remove subject line message message sent compliance federal legislation commercial e mail h r 4176 section 101 paragraph e 1 bill 1618 title iii passed 105 th u congress logo trademark property respective owner product may exactly shown unsubscribe computron 8 follow link click link copy paste address browser',\n",
       "  {'label': ['__label__spam'], 'prob': [0.9883328080177307]})]"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return predictions\n",
    "pipeline_endpoint_name=\"spam-classifier-btext\"\n",
    "#source default session parameters (region, default S3 bucket etc)\n",
    "# region = boto3.Session().region_name\n",
    "# sagemaker_session = sagemaker.Session()\n",
    "# s3_client = boto3.client('s3', region_name=region)\n",
    "# sagemaker_client = boto3.client(\"sagemaker-runtime\")\n",
    "get_predictions(payload, pipeline_endpoint_name, sagemaker_client)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
